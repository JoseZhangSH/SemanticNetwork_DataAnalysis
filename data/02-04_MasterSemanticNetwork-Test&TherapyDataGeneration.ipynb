{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "# 网络分析与可视化\n",
    "import networkx as nx\n",
    "from pyvis import network as net\n",
    "import matplotlib.pyplot as plt\n",
    "import powerlaw # Power laws are probability distributions with the form:p(x)∝x−α"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Rel</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>familiarity</th>\n",
       "      <th>concretness</th>\n",
       "      <th>上级类别</th>\n",
       "      <th>下级类别</th>\n",
       "      <th>coverage</th>\n",
       "      <th>cue_validity</th>\n",
       "      <th>categorical_distinctiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>安全带</td>\n",
       "      <td>可以</td>\n",
       "      <td>可以-保护</td>\n",
       "      <td>27.096774</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>人造物</td>\n",
       "      <td>工具</td>\n",
       "      <td>0.851345</td>\n",
       "      <td>0.038320</td>\n",
       "      <td>0.158537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>安全带</td>\n",
       "      <td>是</td>\n",
       "      <td>是-带子</td>\n",
       "      <td>22.258065</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>人造物</td>\n",
       "      <td>工具</td>\n",
       "      <td>0.851345</td>\n",
       "      <td>0.286019</td>\n",
       "      <td>0.012195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>安全带</td>\n",
       "      <td>其他</td>\n",
       "      <td>其他-车</td>\n",
       "      <td>17.419355</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>人造物</td>\n",
       "      <td>工具</td>\n",
       "      <td>0.851345</td>\n",
       "      <td>0.122012</td>\n",
       "      <td>0.021341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>安全带</td>\n",
       "      <td>是</td>\n",
       "      <td>是-安保的</td>\n",
       "      <td>16.451613</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>人造物</td>\n",
       "      <td>工具</td>\n",
       "      <td>0.851345</td>\n",
       "      <td>0.089938</td>\n",
       "      <td>0.041159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>安全带</td>\n",
       "      <td>可以</td>\n",
       "      <td>可以-保护人</td>\n",
       "      <td>15.483871</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>人造物</td>\n",
       "      <td>工具</td>\n",
       "      <td>0.851345</td>\n",
       "      <td>0.149063</td>\n",
       "      <td>0.035061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Concept Rel Feature  Frequency  familiarity  concretness 上级类别 下级类别  \\\n",
       "0     安全带  可以   可以-保护  27.096774          9.0          9.0  人造物   工具   \n",
       "1     安全带   是    是-带子  22.258065          9.0          9.0  人造物   工具   \n",
       "2     安全带  其他    其他-车  17.419355          9.0          9.0  人造物   工具   \n",
       "3     安全带   是   是-安保的  16.451613          9.0          9.0  人造物   工具   \n",
       "4     安全带  可以  可以-保护人  15.483871          9.0          9.0  人造物   工具   \n",
       "\n",
       "   coverage  cue_validity  categorical_distinctiveness  \n",
       "0  0.851345      0.038320                     0.158537  \n",
       "1  0.851345      0.286019                     0.012195  \n",
       "2  0.851345      0.122012                     0.021341  \n",
       "3  0.851345      0.089938                     0.041159  \n",
       "4  0.851345      0.149063                     0.035061  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取已经处理好的完整语义网络数据\n",
    "\n",
    "df_complete = pd.read_csv('01_Processed Data/Complete-Data.csv')\n",
    "df_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('02_Graph/MasterConceptNetwork.json') as f:\n",
    "with open('02_Graph/MasterConceptNetwork_Word2Vec-0.62_Baseline-Test.json') as f:\n",
    "    js_graph = json.load(f)\n",
    "    G_MasterConceptNetwork = nx.json_graph.node_link_graph(js_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将子图内的节点按重要性排序，以logspace采样出指定数量的节点\n",
    "def NodeSampling(G,num_of_sample, mode):\n",
    "    # pr = nx.pagerank(G, alpha=0.85,weight='weight')\n",
    "    # words = list(dict(sorted(pr.items(), key=lambda item: item[1], reverse=True)))\n",
    "    words = list(sorted(G.nodes(), key=lambda n: G.nodes[n]['closeness_centrality'], reverse=True))\n",
    "    if len(words) > num_of_sample:\n",
    "        if mode == 'log':\n",
    "            # np.logspace(start, end, num_of_samples, endpoint=True, base=10.0)\n",
    "                index_list = np.logspace(0, math.log(len(words)-1,10), num_of_sample, endpoint=True)\n",
    "                index_list = sorted(set([int(x) for x in index_list]))\n",
    "                print(index_list)\n",
    "                words = [words[i] for i in index_list]\n",
    "        elif mode == 'top':\n",
    "                words = words[0:num_of_sample]\n",
    "\n",
    "    return words\n",
    "\n",
    "# 查看全部节点在某个属性的所有选项\n",
    "def NodeAttributeValueList(G,attribute):\n",
    "    return set(np.array([G.nodes[n][attribute] for n in G.nodes]).flatten())\n",
    "\n",
    "# 类别子图\n",
    "def ClusterFilter(G,nodeAttribute, nodeValue, edgeAttribute, edgeValue, writeFile, format):\n",
    "\n",
    "    # 筛选子图 Node & Edge Attribute Filter\n",
    "    def filter_node(node):\n",
    "        if G.nodes[node][nodeAttribute] == nodeValue:\n",
    "            return node \n",
    "\n",
    "    def filter_edge(u,v):\n",
    "        if G[u][v][edgeAttribute] > edgeValue:\n",
    "            return G[u][v]\n",
    "\n",
    "    view = nx.subgraph_view(G,filter_node=filter_node, filter_edge=filter_edge,)\n",
    "    largest_cc = max(nx.connected_components(view), key=len)\n",
    "    view = view.subgraph(largest_cc)\n",
    "\n",
    "    # 储存文件\n",
    "    if writeFile == True:\n",
    "        if format == 'Gephi':\n",
    "            fileName = '02_Graph/ConceptNetwork_'+nodeAttribute+'_'+str(nodeValue)+'_'+edgeAttribute+str(edgeValue)+'.gexf'\n",
    "            nx.write_gexf(view, fileName)\n",
    "\n",
    "        else:\n",
    "            fileName = '02_Graph/ConceptNetwork_'+nodeAttribute+'_'+str(nodeValue)+'_'+edgeAttribute+str(edgeValue)+'.json'\n",
    "            # fileName = '02_Graph/Wenyue/ConceptNetwork_'+nodeAttribute+'_'+str(nodeValue)+'.json'\n",
    "            with open(fileName,'w+') as f:\n",
    "                if format == 'G6':\n",
    "                    f.write(json.dumps(nx.node_link_data(view), ensure_ascii=False).replace('links','edges'))\n",
    "                if format == 'D3':\n",
    "                    f.write(json.dumps(nx.node_link_data(view), ensure_ascii=False))\n",
    "\n",
    "    # 打印提示\n",
    "    print(\"Number of Nodes:\",len(view.nodes))\n",
    "    # print(\"Node Degree Hist\")\n",
    "    # NodeDegreeHist(view)\n",
    "    # print(\"Edge Weight Hist\")\n",
    "    # EdgeWeightHist(view)\n",
    "    \n",
    "    return view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始基线测试用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个大category20词\n",
    "# 按subcategory的总词数来比例分配词汇\n",
    "# 选每个subcategory中最重要的一些词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'交通工具', '人造物', '动物', '植物', '自然物', '身体部位', '食物'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maincategorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自然物\n",
      "Number of Nodes: 10\n",
      "['石头', '冰块', '宝石', '钻石', '珍珠', '岩石', '水晶', '大理石', '白银', '沙粒']\n",
      "动物\n",
      "Number of Nodes: 169\n",
      "[1, 2, 3, 5, 6, 8, 11, 14, 19, 25, 33, 43, 57, 74, 97, 128, 167]\n",
      "['三文鱼', '鱼', '虾', '乌龟', '鱿鱼', '鲍鱼', '鸭子', '天鹅', '小龙虾', '草鱼', '海龟', '沙丁鱼', '多宝鱼', '蜗牛', '瓢虫', '野兔', '蛀虫']\n",
      "人造物\n",
      "Number of Nodes: 540\n",
      "[1, 2, 3, 5, 7, 10, 14, 19, 27, 38, 53, 73, 102, 143, 199, 278, 387, 539]\n",
      "['水壶', '风衣', '枕头', '羽绒服', '衬衫', '盘子', '杯子', '牙签', '杯垫', '礼服', '护手霜', '皮带', '剃须刀', '风扇', '洗手液', '净化器', '刀', '短笛']\n",
      "食物\n",
      "Number of Nodes: 178\n",
      "[1, 2, 3, 5, 6, 8, 11, 15, 20, 26, 34, 45, 59, 78, 102, 134, 176]\n",
      "['红酒', '鸡肉', '火腿肠', '番茄酱', '香肠', '调料', '果酱', '红茶', '红糖', '红烧肉', '冰红茶', '果冻', '荞麦', '蜂蜜', '面粉', '奶酪', '蔗糖']\n",
      "身体部位\n",
      "Number of Nodes: 32\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 15, 18, 21, 25, 30]\n",
      "['胡须', '牙齿', '头发', '皮肤', '脚趾', '膝盖', '头', '耳朵', '手指', '关节', '脚后跟', '眉毛', '头皮', '眼睛', '拇指']\n",
      "交通工具\n",
      "Number of Nodes: 46\n",
      "[1, 2, 3, 4, 6, 7, 9, 11, 13, 16, 20, 24, 30, 36, 44]\n",
      "['汽车', '自行车', '飞机', '房车', '马车', '飞船', '直升机', '三轮车', '游艇', '手推车', '轿车', '消防车', '碰碰车', '航母', '皮艇']\n",
      "植物\n",
      "Number of Nodes: 190\n",
      "[1, 2, 3, 5, 6, 9, 11, 15, 20, 27, 36, 47, 62, 82, 108, 143, 188]\n",
      "['红薯', '黄瓜', '西红柿', '胡萝卜', '西兰花', '莲藕', '蚕豆', '苹果', '海带', '番茄', '卷心菜', '土豆', '杨桃', '豌豆', '四季豆', '牡丹', '杉木']\n"
     ]
    }
   ],
   "source": [
    "cue_words = []\n",
    "\n",
    "maincategorys = NodeAttributeValueList(G_MasterConceptNetwork,'maincategory')\n",
    "for i in maincategorys:\n",
    "    print(i)\n",
    "    view = ClusterFilter(G_MasterConceptNetwork,'maincategory',i,'weight',0,True,'Gephi')\n",
    "    # subcategorys = NodeAttributeValueList(view,'subcategory')\n",
    "    # for j in subcategorys:\n",
    "    #     nodes = [x for x,y in view.nodes(data=True) if y['subcategory']==j]\n",
    "    #     print(j, len(nodes))\n",
    "    cue_words_maincategory = NodeSampling(view,20,'log')\n",
    "    cue_words = cue_words + cue_words_maincategory\n",
    "    print(cue_words_maincategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G_MasterConceptNetwork' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0858abb334f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msubcategorys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_MasterConceptNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mconcepts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmaincategorys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_MasterConceptNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maincategory'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'G_MasterConceptNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "# 完整的词汇列表\n",
    "concepts = []\n",
    "maincategorys = []\n",
    "subcategorys = []\n",
    "\n",
    "for i in list(G_MasterConceptNetwork.nodes()):\n",
    "    concepts.append(i)\n",
    "    maincategorys.append(G_MasterConceptNetwork.nodes()[i]['maincategory'])\n",
    "    subcategorys.append(G_MasterConceptNetwork.nodes()[i]['subcategory'])\n",
    "\n",
    "df_nodes = pd.DataFrame(concepts,columns=[\"concept\"])\n",
    "df_nodes['maincategory'] = maincategorys\n",
    "df_nodes['subcategory'] = subcategorys\n",
    "df_nodes = df_nodes.sort_values(['maincategory','subcategory'])\n",
    "# df_nodes.to_csv('02_Graph/G_MasterConceptNetwork_nodes.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_full_current = ['蕨菜',\n",
    " '青菜',\n",
    " '橄榄',\n",
    " '生菜',\n",
    " '番茄',\n",
    " '卷心菜',\n",
    " '花菜',\n",
    " '大头菜',\n",
    " '西兰花',\n",
    " '蓝莓',\n",
    " '茼蒿',\n",
    " '红薯',\n",
    " '花',\n",
    " '笋',\n",
    " '草',\n",
    " '仙人掌',\n",
    " '油麦菜',\n",
    " '莴笋',\n",
    " '草莓',\n",
    " '浮萍',\n",
    " '煎蛋卷',\n",
    " '奶酪',\n",
    " '牛奶',\n",
    " '肉丸',\n",
    " '饼干',\n",
    " '油饼',\n",
    " '蛋卷',\n",
    " '火腿肠',\n",
    " '奶油',\n",
    " '清汤',\n",
    " '火腿',\n",
    " '香肠',\n",
    " '甜点',\n",
    " '巧克力牛奶',\n",
    " '棉花糖',\n",
    " '粥',\n",
    " '馒头',\n",
    " '腊肠',\n",
    " '酸奶',\n",
    " '烤排骨',\n",
    " '刺猬',\n",
    " '鲤鱼',\n",
    " '猪',\n",
    " '海豹',\n",
    " '豚鼠',\n",
    " '狐狸',\n",
    " '鲸鱼',\n",
    " '海狮',\n",
    " '松鼠',\n",
    " '狼',\n",
    " '犀牛',\n",
    " '鲶鱼',\n",
    " '美洲狮',\n",
    " '猴子',\n",
    " '老虎',\n",
    " '羊驼',\n",
    " '网纹蟒',\n",
    " '熊',\n",
    " '鲫鱼',\n",
    " '黄鼠狼',\n",
    " '便盆',\n",
    " '漏斗',\n",
    " '清洁球',\n",
    " '针线篮',\n",
    " '桶',\n",
    " '陀螺',\n",
    " '扫帚',\n",
    " '猫砂',\n",
    " '簸箕',\n",
    " '管道',\n",
    " '储蓄罐',\n",
    " '沙漏',\n",
    " '万花筒',\n",
    " '骨灰盒',\n",
    " '积木',\n",
    " '地图',\n",
    " '滑轮',\n",
    " '排水管',\n",
    " '哑铃',\n",
    " '壁画',\n",
    " '汽车',\n",
    " '卡车',\n",
    " '吉普车',\n",
    " '赛车',\n",
    " '轿车',\n",
    " '跑车',\n",
    " '车辆',\n",
    " '豪华轿车',\n",
    " '公交车',\n",
    " '摩托车',\n",
    " '电动汽车',\n",
    " '卡丁车',\n",
    " '出租车',\n",
    " '三轮车',\n",
    " '巴士',\n",
    " '敞篷跑车',\n",
    " '房车',\n",
    " '自行车',\n",
    " '行李车',\n",
    " '手推车',\n",
    " '沟渠',\n",
    " '岩石',\n",
    " '水晶',\n",
    " '宝石',\n",
    " '月亮',\n",
    " '沙坑',\n",
    " '煤田',\n",
    " '大理石',\n",
    " '钻石',\n",
    " '雪花',\n",
    " '煤炭',\n",
    " '流星',\n",
    " '白银',\n",
    " '珍珠',\n",
    " '冰雹',\n",
    " '星星',\n",
    " '露珠',\n",
    " '沙尘暴'\n",
    " '石头',\n",
    " '雪球',\n",
    " '臀部',\n",
    " '肩膀',\n",
    " '脚趾',\n",
    " '脚背',\n",
    " '嘴唇',\n",
    " '拇指',\n",
    " '膝盖',\n",
    " '大腿',\n",
    " '心脏',\n",
    " '腿',\n",
    " '关节',\n",
    " '手臂',\n",
    " '脚后跟',\n",
    " '血管',\n",
    " '手指',\n",
    " '脚',\n",
    " '牙齿',\n",
    " '喉',\n",
    " '手掌',\n",
    " '睫毛']\n",
    "\n",
    " # 得到反馈要删除的词汇\n",
    "\n",
    "list_delete_current = ['蕨菜',\n",
    " '橄榄',\n",
    " '生菜',\n",
    " '大头菜',\n",
    " '茼蒿',\n",
    " '花',\n",
    " '油麦菜',\n",
    " '浮萍',\n",
    " '煎蛋卷',\n",
    " '油饼',\n",
    " '奶油',\n",
    " '清汤',\"\"\n",
    " '火腿',\n",
    " '甜点',\n",
    " '巧克力牛奶',\n",
    " '腊肠',\n",
    " '烤排骨',\n",
    " '鲤鱼',\n",
    " '豚鼠',\n",
    " '海狮',\n",
    " '鲶鱼',\n",
    " '羊驼',\n",
    " '网纹蟒',\n",
    " '鲫鱼',\n",
    " '黄鼠狼',\n",
    " '便盆',\n",
    " '清洁球',\n",
    " '猫砂',\n",
    " '簸箕',\n",
    " '管道',\n",
    " '万花筒',\n",
    " '骨灰盒',\n",
    " '地图',\n",
    " '滑轮',\n",
    " '排水管',\n",
    " '壁画',\n",
    " '汽车',\n",
    " '跑车',\n",
    " '车辆',\n",
    " '豪华轿车',\n",
    " '电动汽车',\n",
    " '卡丁车',\n",
    " '敞篷跑车',\n",
    " '房车',\n",
    " '行李车',\n",
    " '沟渠',\n",
    " '宝石',\n",
    " '沙坑',\n",
    " '煤田',\n",
    " '煤炭',\n",
    " '臀部',\n",
    " '肩膀',\n",
    " '脚趾',\n",
    " '脚背',\n",
    " '大腿',\n",
    " '关节',\n",
    " '脚后跟',\n",
    " '手指',\n",
    " '睫毛']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_valid_current = list(set(list_full_current) ^ set(list_delete_current))\n",
    "list_valid_next = list(set(concepts).intersection(set(list_valid_current)))\n",
    "list(set(list_valid_next).intersection(set(cue_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_checklist_next = set(cue_words).union(set(list_valid_next)).difference(set(list_delete_current))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodes[df_nodes['concept'].isin(list_checklist_next)].to_csv(\"02_VASystemData/check_list.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成命名测试用数据 Picture Naming Test Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('02_VASystemData/cue_words.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    cue_words = [x.replace('\\ufeff','') for x in sum(list(reader),[])]\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取基线测试词汇列表\n",
    "df = pd.read_excel('02_VASystemData/Baseline_CueWords.xlsx').dropna()\n",
    "cue_words = list(df.concept.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input \n",
    "# word_list = ['豆浆', '菠萝汁', '咖啡']\n",
    "word_list = cue_words\n",
    "\n",
    "data = {\n",
    "    'currentStep': 0,\n",
    "    'steps': []\n",
    "}\n",
    "\n",
    "for word in word_list:\n",
    "    new_step = {\n",
    "        'name':word,\n",
    "        'image':'/SemanticNetwork_DataCollectionTools/test_images/'+word+'.jpeg',\n",
    "        'countdown':20,\n",
    "        'result':'fail',\n",
    "        'status':'unchecked',\n",
    "    }\n",
    "    data['steps'].append(new_step)\n",
    "\n",
    "with open(\"02_VASystemData/test_picture-naming.json\", \"w\") as f: \n",
    "    json.dump(data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成语义特征分析训练用数据 SFA Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " #列出共有某个语义特征的同类概念\n",
    "\n",
    "def List_Related_Concepts(concept,feature):\n",
    "    main_category = df_complete[df_complete['Concept'] == concept]['上级类别'].unique()[0]\n",
    "    related_concepts = list(df_complete[(df_complete['上级类别'] == main_category) & (df_complete['Feature'] == feature)]['Concept'].unique())\n",
    "    related_concepts = list(filter(lambda x: x != concept, related_concepts))\n",
    "\n",
    "    return related_concepts\n",
    "\n",
    "# List_Related_Concepts('安全带','可以-保护人')\n",
    "\n",
    "\n",
    "# 易混淆的语义特征\n",
    "# 给到（概念，语义特征）\n",
    "#   选出所有的同类概念，选出所有同类语义特征，计算每个同类语义特征在该类概念中的线索度\n",
    "#     按照线索度高低排序\n",
    "#         如果该特征不为该概念所有\n",
    "#             则作为混淆项\n",
    "\n",
    "def List_Confused_Feature(concept, feature):\n",
    "    # 改成同Subcategory\n",
    "    sub_category = df_complete[df_complete['Concept'] == concept]['下级类别'].unique()[0]\n",
    "    related_concepts = list(df_complete[df_complete['下级类别'] == sub_category]['Concept'].unique())\n",
    "    related_concepts = list(filter(lambda x: x != concept, related_concepts))\n",
    "\n",
    "    # related_concepts = List_Related_Concepts(concept,feature)\n",
    "    df_related = df_complete[(df_complete['Concept'].isin(related_concepts))]\n",
    "\n",
    "    relationship = df_complete[(df_complete['Concept'] == concept) & (df_complete['Feature'] == feature)]['Rel'].unique()[0]\n",
    "    confused_feature_list = list(df_related[df_related['Rel'] == relationship].sort_values(by=['categorical_distinctiveness','cue_validity'],ascending=False)['Feature'].unique())\n",
    "    confused_feature_list = list(filter(lambda x: x not in list(df_complete[df_complete['Concept'] == concept]['Feature']), confused_feature_list))\n",
    "\n",
    "    return confused_feature_list\n",
    "\n",
    "# List_Confused_Feature('安全带','可以-保护人')[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input \n",
    "word_list = ['轿车',\n",
    "  '摩托车',\n",
    "  '消防车',\n",
    "  '消防栓',\n",
    "  '水',\n",
    "  '冰红茶',\n",
    "  '谷子',\n",
    "  '薄荷',\n",
    "  '白菜']\n",
    "# word_list = cue_words\n",
    "\n",
    "\n",
    "data = {\n",
    " \"steps\": []\n",
    "}\n",
    "for word in word_list:\n",
    "    new_step = {\n",
    "        'result':'',\n",
    "        'status':'unchecked',\n",
    "        \"graph\": {\n",
    "            \"id\": \"root\",\n",
    "            \"label\": word,\n",
    "            'img':'/SemanticNetwork_DataCollectionTools/test_images/'+word+'.jpeg',\n",
    "            # 'img':'/test_images/'+word+'.jpeg',\n",
    "\n",
    "            \"children\": []\n",
    "        },\n",
    "        \"mention\": {\n",
    "            \"属于\":[],\n",
    "            \"用于\":[],\n",
    "            \"做\":[],\n",
    "            \"有\":[],\n",
    "            \"在\":[],\n",
    "            \"联想到\":[],\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    concept = word\n",
    "    maincategory = df_complete[df_complete['Concept'] == concept]['上级类别'].unique()[0]\n",
    "    mention_belongto = list(df_complete[df_complete['上级类别'] == maincategory]['下级类别'].unique())[0:3]\n",
    "    # rel_list = list(df_complete['Rel'].unique())\n",
    "\n",
    "    feature_function = df_complete[(df_complete['Concept']==concept)&(df_complete['Rel']=='可以')].sort_values(by=['cue_validity'],ascending=False)['Feature'].unique()[0]\n",
    "    mention_function = List_Confused_Feature(concept,feature_function)[0:3]\n",
    "    mention_function.append(feature_function)\n",
    "\n",
    "    feature_need = df_complete[(df_complete['Concept']==concept)&(df_complete['Rel']=='需要')].sort_values(by=['cue_validity'],ascending=False)['Feature'].unique()[0]\n",
    "    mention_need = List_Confused_Feature(concept,feature_need)[0:3]\n",
    "    mention_need.append(feature_need)\n",
    "\n",
    "    feature_have = df_complete[(df_complete['Concept']==concept)&(df_complete['Rel']=='有')].sort_values(by=['cue_validity'],ascending=False)['Feature'].unique()[0]\n",
    "    mention_have = List_Confused_Feature(concept,feature_have)[0:3]\n",
    "    mention_have.append(feature_have)\n",
    "\n",
    "    feature_similar = df_complete[(df_complete['Concept']==concept)&(df_complete['Rel']=='像')].sort_values(by=['cue_validity'],ascending=False)['Feature'].unique()[0]\n",
    "    mention_similar = List_Confused_Feature(concept,feature_similar)\n",
    "    if '像-'+concept in mention_similar:\n",
    "        mention_similar.remove('像-'+concept)\n",
    "    mention_similar=mention_similar[0:3]\n",
    "    mention_similar.append(feature_similar)\n",
    "\n",
    "    new_step['mention']['属于'] = mention_belongto\n",
    "    new_step['mention']['用于'] = [i.replace('可以-','')  for i in mention_function]\n",
    "    new_step['mention']['做'] = [i.replace('需要-','')  for i in mention_need]\n",
    "    new_step['mention']['有'] = [i.replace('有-','')  for i in mention_have]\n",
    "    new_step['mention']['联想到'] = [i.replace('像-','')  for i in mention_similar]\n",
    "    \n",
    "    data['steps'].append(new_step)\n",
    "\n",
    "\n",
    "with open(\"02_VASystemData/test_SFA.json\", \"w\") as f: \n",
    "    json.dump(data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
